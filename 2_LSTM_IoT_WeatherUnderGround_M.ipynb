{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from statistics import mean \n",
    "import time\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "script_path = os.path.abspath(\"pyscripts\")\n",
    "if script_path not in sys.path:\n",
    "    sys.path.append(script_path)\n",
    "from utils import *\n",
    "from load_data import *\n",
    "from lstm import *\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# time sequence\n",
    "train_window = 48\n",
    "input_size = 1\n",
    "output_size = 24\n",
    "hidden_size = 48\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.load('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/Geotab_Obs_Ratio05.npy')\n",
    "# swap rows and columns, obs = np.swapaxes(obs, 0, 1)\n",
    "obs = obs.T\n",
    "times = np.load('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/Geotab_Obs_TS_Ratio05.npy')\n",
    "times = [datetime.utcfromtimestamp(x.tolist()/1e9) for x in times]\n",
    "stations = pd.read_csv('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/selected_Geotab_stations_Ratio05.csv')\n",
    "stations = stations[stations.columns[[3,1,2,4]]]\n",
    "\n",
    "weather_station_2015_2020 = pd.read_csv(\"/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/weather_station_2015_2020.csv\")\n",
    "weather_time_2015_2020 = np.load('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/weather_time_2015_2020.npy', allow_pickle=True)\n",
    "\n",
    "weather_obs_2015_2020 = np.load('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/weather_obs_2015_2020.npy')\n",
    "weather_obs_2010_2020 = np.load('/home/graduate/fbx5002/disk10TB/IEE/IoT_Weather_Data/weather_obs_2010_2020.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geotab length: 8784\n"
     ]
    }
   ],
   "source": [
    "print('geotab length:', len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stations5 = set(stations.sort_values('missing')[:455]['geohash'])\n",
    "stations10= set(stations.sort_values('missing')[:1650]['geohash'])\n",
    "stations15= set(stations.sort_values('missing')[:2107]['geohash'])\n",
    "stations20= set(stations.sort_values('missing')[:2562]['geohash'])\n",
    "# stations25= set(stations.sort_values('missing')[:2949]['geohash'])\n",
    "# stations30= set(stations.sort_values('missing')[:3220]['geohash'])\n",
    "# stations35= set(stations.sort_values('missing')[:3571]['geohash'])\n",
    "# stations40= set(stations.sort_values('missing')[:3943]['geohash'])\n",
    "# stations45= set(stations.sort_values('missing')[:4339]['geohash'])\n",
    "# stations50 = set(stations['geohash'])\n",
    "wu_stations = set(weather_station_2015_2020['station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "after_missingratio = list(map(lambda x: sum(np.isnan(x))/8784, obs))\n",
    "print(\"The data shape of IoT and Weather_Underground data are %s, %s\" %(obs.shape, weather_obs_2015_2020.shape))\n",
    "print(\"The time length of IoT and Weather_Underground data are %s, %s\" % (len(times), len(weather_time_2015_2020)))\n",
    "print(\"Time window of IoT and Weather_Underground data are [%s, %s], [%s, %s]\" % \n",
    "      (times[0].strftime(\"%Y%m%d%H\"), times[-1].strftime(\"%Y%m%d%H\"), \n",
    "       weather_time_2015_2020[0].strftime(\"%Y%m%d\"), weather_time_2015_2020[-1].strftime(\"%Y%m%d\")))\n",
    "print(\"Original Minimum and Maximum Missing Data Ratio for IoT Stations are %s, %s\" % (min(stations['missing']), max(stations['missing'])))\n",
    "print(\"After Interpolation, Minimum and Maximum Missing Data Ratio for IoT Stations are %s, %s\" % (min(after_missingratio), max(after_missingratio)))\n",
    "print(stations.head(5)); print(weather_station_2015_2020.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# select days for testing\n",
    "test_times = []\n",
    "for _time in times:\n",
    "    if _time.hour > 0:\n",
    "        continue\n",
    "    test_times.append(datetime(_time.year, _time.month, _time.day, 0 , 0))\n",
    "random.seed(7)\n",
    "seq = [i for i in range(len(test_times))]\n",
    "random.shuffle(seq) # inplace shuffle\n",
    "test_times = np.array(sorted([test_times[i] for i in seq[:30]]))\n",
    "print(test_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create infos dict and Calculate min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "infos = {}\n",
    "for i, station in enumerate(stations['geohash']):\n",
    "    infos[station] = obs[i]\n",
    "print(list(infos.items())[:2])\n",
    "\n",
    "# operations below will in-place rewrite infos variable\n",
    "norm_min = []\n",
    "norm_max = []\n",
    "for key, value in infos.items():\n",
    "    norm_min.append(min(value))\n",
    "    norm_max.append(max(value))\n",
    "# len(list(infos_filtered.items())[0][1]) # this is the station we used for the single station training\n",
    "norm_min = min(norm_min)\n",
    "norm_max = max(norm_max)\n",
    "print(\"The minimum and maximal temperatures for IoT stations are %.2f, %.2f\" %(norm_min, norm_max))\n",
    "\n",
    "# Observation: the higher interpolation rate, more similar/correlated that two stations data are\n",
    "# why 'dr5nw3r' looks same as 'dr5nw92'\n",
    "'dr5nw3r' in stations20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "infos_w = {}\n",
    "for i, station in enumerate(weather_station_2015_2020['station']):\n",
    "    infos_w[station] = weather_obs_2015_2020[i]\n",
    "print(list(infos_w.items())[:2])\n",
    "\n",
    "# operations below will in-place rewrite infos variable\n",
    "norm_min_w = []\n",
    "norm_max_w = []\n",
    "for key, value in infos_w.items():\n",
    "    norm_min_w.append(min(value))\n",
    "    norm_max_w.append(max(value))\n",
    "# len(list(infos_filtered.items())[0][1]) # this is the station we used for the single station training\n",
    "norm_min_w = min(norm_min_w)\n",
    "norm_max_w = max(norm_max_w)\n",
    "print(\"The minimum and maximal temperatures for Weather Underground stations are %.2f, %.2f\" %(norm_min_w, norm_max_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "norm_min = min(norm_min, norm_min_w)\n",
    "norm_max = max(norm_max, norm_max_w)\n",
    "print(norm_min, norm_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split data for test_day and train_day for IoT stations\n",
    "\n",
    "### key point\n",
    "- only need to make sure the 24 hours output for test_times not seen in the training dataset\n",
    "- the 48 hours input of test_times can be seen in the training dataset\n",
    "- weather underground stations all for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_time_range = []\n",
    "for testtime in test_times:\n",
    "    start = testtime - timedelta(hours = train_window)\n",
    "    end = testtime + timedelta(hours = output_size-1) # include itself\n",
    "    if end > times[-1]:\n",
    "        continue\n",
    "    test_time_range.append((start, end))\n",
    "    \n",
    "test_time_index = sorted([(times.index(x), times.index(y)) for x, y in test_time_range])\n",
    "removed_time_index = sorted([(times.index(x)+train_window, times.index(y)) for x, y in test_time_range])\n",
    "print(test_time_index[:5])\n",
    "print(removed_time_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_data = {}\n",
    "train_data = {}\n",
    "for key, data in list(infos.items()):\n",
    "    test, train = [], []\n",
    "    for test_time in test_time_index:\n",
    "        test.append(data[test_time[0]:(test_time[1]+1)])\n",
    "    \n",
    "    for i in range(len(removed_time_index)):\n",
    "        if i ==0:\n",
    "            end = removed_time_index[0][0]\n",
    "            train.append(data[:end])   \n",
    "        else:\n",
    "            start = removed_time_index[i-1][1]+1\n",
    "            end = removed_time_index[i][0]\n",
    "            train.append(data[start:end])\n",
    "    # add elements after last removed_time_index\n",
    "    start = removed_time_index[len(removed_time_index)-1][1]+1\n",
    "    train.append(data[start:])\n",
    "    # change to nparray\n",
    "    train_data[key] = train\n",
    "    test_data[key] = test\n",
    "\n",
    "#len(infos['dr72rzt'][0]) == len(train_data['dr72rzt'][0]) + len(test_data['dr72rzt'][0]) \n",
    "l1 = sum([len(x) for x in train_data['dr72rzt']])\n",
    "l2 = sum([y-x+1 for x, y in removed_time_index])\n",
    "print(l1, l2)\n",
    "l1+l2 == len(infos['dr72rzt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare training and test tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_data.update(infos_w)\n",
    "train_tensors = Dataset(train_data, (norm_min, norm_max), train_window, output_size)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(\"Training input and output for a IoT station: %s, %s\" % (train_tensors[0][0].shape, train_tensors[0][1].shape))\n",
    "print(\"Validation input and output for a IoT station: %s, %s\" % (train_tensors[0][2].shape, train_tensors[0][3].shape))\n",
    "idx = train_tensors.keys.index('KNJCLIFF7')\n",
    "print(\"Training input and output for a weather station: %s, %s\" % (train_tensors[idx][0].shape, train_tensors[idx][1].shape))\n",
    "print(\"Validation input and output for a weather station: %s, %s\" % (train_tensors[idx][2].shape, train_tensors[idx][3].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_tensors = DatasetTestDays(test_data, (norm_min, norm_max), train_window)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(\"Testing input and output for one IoT station: %s, %s\" % (test_tensors[0][0].shape, test_tensors[0][1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# three selected days have na values in their observations which cannot be predicted\n",
    "missing_days = np.unique(test_tensors.missing)\n",
    "print(missing_days)\n",
    "test_times = np.delete(test_times, missing_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "db = {'train': train_tensors, 'test': test_tensors, 'selected_days': test_times}\n",
    "torch.save(db, './checkpoint/IoT_final/TensorData/Torch_db5WU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Train LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load('./checkpoint/IoT_final/TensorData/Torch_db5WU')\n",
    "train_tensors = loaded['train']\n",
    "test_tensors = loaded['test']\n",
    "test_times = loaded['selected_days']\n",
    "norm_min, norm_max = train_tensors.min, train_tensors.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-16d7efa2f601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# dir() or __dir__:return an alphabetized list of names comprising (some of) the attributes of the given object, and of attributes reachable from it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(dir(list(model.parameters())[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtotalTrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/amethyst/s0/fbx5002/PythonWorkingDir/IEE/pyscripts/lstm.py\u001b[0m in \u001b[0;36minitial_model\u001b[0;34m(input_size, output_size, hidden_size, num_layers, learning_rate, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitial_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# mean-squared error for regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# To compute the number of trainable parameters\n",
    "# dir() or __dir__:return an alphabetized list of names comprising (some of) the attributes of the given object, and of attributes reachable from it\n",
    "# dir() or __dir__:return an alphabetized list of names comprising (some of) the attributes of the given object, and of attributes reachable from it\n",
    "# print(dir(list(model.parameters())[0]))\n",
    "_, model, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "totalTrainable = []\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, p.shape)\n",
    "    # total number of trainable parameters\n",
    "    if p.requires_grad:\n",
    "        totalTrainable.append(p.numel())\n",
    "    else:\n",
    "        totalTrainable.append(None)\n",
    "        \n",
    "print(totalTrainable)\n",
    "\n",
    "num_epochs = 101\n",
    "epoch_interval = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 5.5%\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-5.5-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, device, stationlist = stations5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 10%\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-10-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, device, stationlist = stations10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 15%\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-15-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, device, stationlist = stations15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 20%\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-20-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "         device, stationlist = stations20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize and train the model for 25%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-25-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations25)\n",
    "\n",
    "# # initialize and train the model for 30%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-30-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations30)\n",
    "\n",
    "# # initialize and train the model for 35%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-35-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations35)\n",
    "\n",
    "# # initialize and train the model for 40%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-40-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations40)\n",
    "\n",
    "# # initialize and train the model for 45%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-45-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations45)\n",
    "\n",
    "# # initialize and train the model for 50%\n",
    "# checkpoint_format = 'checkpoint/IoT_final/checkpoint-50-{epoch}.pth.tar'\n",
    "# loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "# Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format, \n",
    "#          device, stationlist = stations50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 5 WU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 5.5%IoT+5WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-5.5-IoT-5-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations5.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 10%IoT+5WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-10-IoT-5-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations10.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 15%IoT+5 WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-15-IoT-5-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations15.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize and train the model for 20%IoT+5 WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-20-IoT-5-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations20.union(wu_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 10 WU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the model for 5.5%IoT+10WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-5.5-IoT-10-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations5.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the model for 10%IoT+10WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-10-IoT-10-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations10.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the model for 15%IoT+10WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-15-IoT-10-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations15.union(wu_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the model for 20%IoT+10WU\n",
    "checkpoint_format = 'checkpoint/IoT_final/checkpoint-20-IoT-10-W-{epoch}.pth.tar'\n",
    "loss_func, model, optimizer = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device) \n",
    "Training(loss_func, model, optimizer, train_tensors, num_epochs, epoch_interval, checkpoint_format,\n",
    "         device, stationlist = stations20.union(wu_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Predict and Statistic Analysis_SplitByDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load('./checkpoint/IoT_final/TensorData/Torch_db5WU')\n",
    "train_tensors = loaded['train']\n",
    "test_tensors = loaded['test']\n",
    "test_times = loaded['selected_days']\n",
    "norm_min, norm_max = train_tensors.min, train_tensors.max\n",
    "\n",
    "print(norm_min, norm_max)\n",
    "\n",
    "print(\"Training input and output for a IoT station: %s, %s\" % (train_tensors[0][0].shape, train_tensors[0][1].shape))\n",
    "print(\"Validation input and output for a IoT station: %s, %s\" % (train_tensors[0][2].shape, train_tensors[0][3].shape))\n",
    "idx = train_tensors.keys.index('KNJCLIFF7')\n",
    "print(\"Training input and output for a weather station: %s, %s\" % (train_tensors[idx][0].shape, train_tensors[idx][1].shape))\n",
    "print(\"Validation input and output for a weather station: %s, %s\" % (train_tensors[idx][2].shape, train_tensors[idx][3].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# read the saved model and use the best one for predicting\n",
    "checkpoint5 = torch.load('./checkpoint/IoT_final/checkpoint-5.5-{epoch}.pth.tar'.format(epoch=60))\n",
    "checkpoint10 = torch.load('./checkpoint/IoT_final/checkpoint-10-{epoch}.pth.tar'.format(epoch=23))\n",
    "checkpoint15 = torch.load('./checkpoint/IoT_final/checkpoint-15-{epoch}.pth.tar'.format(epoch=8))\n",
    "checkpoint20 = torch.load('./checkpoint/IoT_final/checkpoint-20-{epoch}.pth.tar'.format(epoch=10))\n",
    "# checkpoint25 = torch.load('./checkpoint/IoT_final/checkpoint-25-{epoch}.pth.tar'.format(epoch=4))\n",
    "# checkpoint30 = torch.load('./checkpoint/IoT_final/checkpoint-30-{epoch}.pth.tar'.format(epoch=6))\n",
    "# checkpoint35 = torch.load('./checkpoint/IoT_final/checkpoint-35-{epoch}.pth.tar'.format(epoch=7))\n",
    "# checkpoint40 = torch.load('./checkpoint/IoT_final/checkpoint-40-{epoch}.pth.tar'.format(epoch=9))\n",
    "# checkpoint45 = torch.load('./checkpoint/IoT_final/checkpoint-45-{epoch}.pth.tar'.format(epoch=6))\n",
    "# checkpoint50 = torch.load('./checkpoint/IoT_final/checkpoint-50-{epoch}.pth.tar'.format(epoch=3))\n",
    "\n",
    "checkpoint5IoT10WU = torch.load('./checkpoint/IoT_final/checkpoint-5.5-IoT-10-W-{epoch}.pth.tar'.format(epoch=19))\n",
    "checkpoint10IoT10WU = torch.load('./checkpoint/IoT_final/checkpoint-10-IoT-10-W-{epoch}.pth.tar'.format(epoch=11))\n",
    "checkpoint15IoT10WU = torch.load('./checkpoint/IoT_final/checkpoint-15-IoT-10-W-{epoch}.pth.tar'.format(epoch=4))\n",
    "checkpoint20IoT10WU = torch.load('./checkpoint/IoT_final/checkpoint-20-IoT-10-W-{epoch}.pth.tar'.format(epoch=7))\n",
    "\n",
    "checkpoint5IoT5WU = torch.load('./checkpoint/IoT_final/checkpoint-5.5-IoT-5-W-{epoch}.pth.tar'.format(epoch=18))\n",
    "checkpoint10IoT5WU = torch.load('./checkpoint/IoT_final/checkpoint-10-IoT-5-W-{epoch}.pth.tar'.format(epoch=9))\n",
    "checkpoint15IoT5WU = torch.load('./checkpoint/IoT_final/checkpoint-15-IoT-5-W-{epoch}.pth.tar'.format(epoch=5))\n",
    "checkpoint20IoT5WU = torch.load('./checkpoint/IoT_final/checkpoint-20-IoT-5-W-{epoch}.pth.tar'.format(epoch=7))\n",
    "\n",
    "# initialize and train the model\n",
    "_, model5, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model10, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model15, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model20, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model25, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model30, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model35, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model40, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model45, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "# _, model50, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "\n",
    "_, model5IoT10WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model10IoT10WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model15IoT10WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model20IoT10WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "\n",
    "_, model5IoT5WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model10IoT5WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model15IoT5WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "_, model20IoT5WU, _ = initial_model(input_size, output_size, hidden_size, num_layers, learning_rate, device)\n",
    "\n",
    "model5.load_state_dict(checkpoint5['bestLossModel'])\n",
    "model10.load_state_dict(checkpoint10['bestLossModel'])\n",
    "model15.load_state_dict(checkpoint15['bestLossModel'])\n",
    "model20.load_state_dict(checkpoint20['bestLossModel'])\n",
    "# model25.load_state_dict(checkpoint25['bestLossModel'])\n",
    "# model30.load_state_dict(checkpoint30['bestLossModel'])\n",
    "# model35.load_state_dict(checkpoint35['bestLossModel'])\n",
    "# model40.load_state_dict(checkpoint40['bestLossModel'])\n",
    "# model45.load_state_dict(checkpoint45['bestLossModel'])\n",
    "# model50.load_state_dict(checkpoint50['bestLossModel'])\n",
    "\n",
    "model5IoT10WU.load_state_dict(checkpoint5IoT10WU['bestLossModel'])\n",
    "model10IoT10WU.load_state_dict(checkpoint10IoT10WU['bestLossModel'])\n",
    "model15IoT10WU.load_state_dict(checkpoint15IoT10WU['bestLossModel'])\n",
    "model20IoT10WU.load_state_dict(checkpoint20IoT10WU['bestLossModel'])\n",
    " \n",
    "model5IoT5WU.load_state_dict(checkpoint5IoT5WU['bestLossModel'])\n",
    "model10IoT5WU.load_state_dict(checkpoint10IoT5WU['bestLossModel'])\n",
    "model15IoT5WU.load_state_dict(checkpoint15IoT5WU['bestLossModel'])\n",
    "model20IoT5WU.load_state_dict(checkpoint20IoT5WU['bestLossModel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save into npy\n",
    "tests = ['model5', 'model10', 'model15', 'model20', \n",
    "         'model25', 'model30', 'model35', 'model40', 'model45', 'model50',\n",
    "         'model5IoT5WU', 'model10IoT5WU', 'model15IoT5WU', 'model20IoT5WU',\n",
    "        'model5IoT10WU', 'model10IoT10WU', 'model15IoT10WU', 'model20IoT10WU']\n",
    "models = [model5, model10, model15, model20, \n",
    "          model25, model30, model35, model40, model45, model50,\n",
    "          model5IoT5WU, model10IoT5WU, model15IoT5WU, model20IoT5WU,\n",
    "         model5IoT10WU, model10IoT10WU, model15IoT10WU, model20IoT10WU]\n",
    "stations_ = [stations5, stations10, stations15, stations20, stations5, stations10, stations15, stations20]\n",
    "\n",
    "for i in range(len(tests)):\n",
    "    output_folder = 'IoT_Weather_Data/output/testOnStation5/'+tests[i]+'/'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    test_pred_orig_dict = predict(models[i], test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "\n",
    "    # test_stations are not shuffled\n",
    "    outputStations = stations[stations['geohash'].isin(list(test_pred_orig_dict.keys()))]\n",
    "    list(outputStations['geohash'])== list(test_pred_orig_dict.keys())\n",
    "\n",
    "    outputStations.to_csv(output_folder+'stations.csv', index=False)\n",
    "    np.save(output_folder+'selectedDates.npy', test_times)\n",
    "    np.save(output_folder+'predictions.npy', np.array([x[0].numpy() for x in list(test_pred_orig_dict.values())]))\n",
    "    np.save(output_folder+'observations.npy', np.array([x[1].numpy() for x in list(test_pred_orig_dict.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "checkpoints = [checkpoint5, checkpoint10, checkpoint15, checkpoint20]\n",
    "stations_ = [stations5, stations10, stations15, stations20]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
    "for i, ax in enumerate(axs):   \n",
    "    ax.plot(meanLossEpoch(checkpoints[i]['train_loss'], len(stations_[i])))\n",
    "    ax.plot(meanLossEpoch(checkpoints[i]['test_loss'], len(stations_[i])))\n",
    "    ax.set_ylim([0, 0.0025])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend([\"Training\", \"Validation\"])\n",
    "plt.savefig('LossThreeRatios.jpeg', dpi=300)\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "checkpoints = [checkpoint5IoTWU, checkpoint10IoTWU, checkpoint15IoTWU, checkpoint20IoTWU]\n",
    "stations_ = [stations5, stations10, stations15, stations20]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
    "for i, ax in enumerate(axs):   \n",
    "    ax.plot(meanLossEpoch(checkpoints[i]['train_loss'], len(stations_[i])))\n",
    "    ax.plot(meanLossEpoch(checkpoints[i]['test_loss'], len(stations_[i])))\n",
    "    ax.set_ylim([0, 0.0025])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend([\"Training\", \"Validation\"])\n",
    "plt.savefig('LossThreeRatios_IOT_WU.jpeg', dpi=300)\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare four models on 5.5% stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_pred_orig_dict5 = predict(model5, test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "test_pred_orig_dict10 = predict(model10, test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "test_pred_orig_dict15 = predict(model15, test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "test_pred_orig_dict20 = predict(model20, test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "\n",
    "# calculate root mean squared error\n",
    "testScores_stations5, test_min5, test_mean5, test_max5 = stat_scores(test_pred_orig_dict5)\n",
    "testScores_stations10, test_min10, test_mean10, test_max10 = stat_scores(test_pred_orig_dict10)\n",
    "testScores_stations20, test_min20, test_mean20, test_max20= stat_scores(test_pred_orig_dict20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_pred_orig_dict = predict(modelIoTWU, test_tensors, (norm_min, norm_max), device, stationlist = stations5)\n",
    "# calculate root mean squared error\n",
    "testScores_stations, test_min, test_mean, test_max = stat_scores(test_pred_orig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # ##### plot baseline and predictions for test station having max RMSE\n",
    "# # fig, axs = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(14,10))\n",
    "# # i  =0\n",
    "# # for row in axs:\n",
    "# #     for col in row:\n",
    "# #         col.plot(test_pred_orig_dict[test_mean][1][i]) # original\n",
    "# #         col.plot(test_pred_orig_dict[test_mean][0][i], '--') # predicted\n",
    "# #         i+=1\n",
    "\n",
    "# errors_rmse = list(map(lambda x, y : math.sqrt(mean_squared_error(x, y)), \n",
    "#                                 test_pred_orig_dict[test_mean][0],test_pred_orig_dict[test_mean][1]))\n",
    "# errors_mae = list(map(lambda x, y : mean(abs(x- y).numpy()), \n",
    "#                                 test_pred_orig_dict[test_mean][0],test_pred_orig_dict[test_mean][1]))\n",
    "# print(sorted(errors_rmse, reverse=True))\n",
    "# print(sorted(errors_mae, reverse=True))\n",
    "# print(\"\\n\")\n",
    "# print(\"Average RMSE over 27 selected days is %s\" % mean(errors_rmse))\n",
    "# idx = [i for i, error in enumerate(errors_rmse) if error > 1.5*mean(errors_rmse)]\n",
    "# outlier_times = sorted([test_times[i] for i in idx])\n",
    "# print(outlier_times)\n",
    "\n",
    "# weights = np.ones_like(errors_rmse) / (len(errors_rmse))\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "# ax1.hist(errors_rmse, bins = 20, range=(0.5, 8), edgecolor = 'black', weights= weights)\n",
    "# ax1.set_xlabel('Average RMSE for Each Selected Day')\n",
    "# ax1.set_ylabel('Percentage')\n",
    "# ax1.set_ylim(0, 0.3)\n",
    "# ax2.hist(errors_mae, bins = 20, range=(0.5, 8), edgecolor = 'black', weights= weights)\n",
    "# ax2.set_xlabel('Average MAE for Each Selected Day')\n",
    "# ax2.set_ylabel('Percentage')\n",
    "# plt.savefig('HistRMSEDays.jpeg', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# testScores_stations = dict()\n",
    "# idx = [error < 1.5*mean(errors_rmse) for error in errors_rmse]\n",
    "# testScores_stations, test_min_, test_mean_, test_max_  = stat_scores(test_pred_orig_dict, idx)\n",
    "# print(\"Test Stations with min, mean and max RMSE are %s, %s, %s\" % (test_min_, test_mean_, test_max_))\n",
    "\n",
    "# # mae over predicted time points for test station with mean error\n",
    "# over_time_mae = np.mean(abs(test_pred_orig_dict[test_mean][0][idx] - test_pred_orig_dict[test_mean][1][idx]).numpy(), axis =0)\n",
    "# print(over_time_mae)\n",
    "# plt.plot(over_time_mae)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model10 and model IoTWU to predict on 10% stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_pred_orig_dict10 = predict(model10, test_tensors, (norm_min, norm_max), device, stationlist = stations10)\n",
    "# calculate root mean squared error\n",
    "testScores_stations10, test_min10, test_mean10, test_max10 = stat_scores(test_pred_orig_dict10)\n",
    "\n",
    "test_pred_orig_dict = predict(modelIoTWU, test_tensors, (norm_min, norm_max), device, stationlist = stations10)\n",
    "# calculate root mean squared error\n",
    "testScores_stations, test_min, test_mean, test_max = stat_scores(test_pred_orig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "errors_rmse = list(map(lambda x, y : math.sqrt(mean_squared_error(x, y)), \n",
    "                                test_pred_orig_dict[test_mean][0],test_pred_orig_dict[test_mean][1]))\n",
    "errors_mae = list(map(lambda x, y : mean(abs(x- y).numpy()), \n",
    "                                test_pred_orig_dict[test_mean][0],test_pred_orig_dict[test_mean][1]))\n",
    "print(sorted(errors_rmse, reverse=True))\n",
    "print(sorted(errors_mae, reverse=True))\n",
    "print(\"\\n\")\n",
    "print(\"Average RMSE over 27 selected days is %s\" % mean(errors_rmse))\n",
    "idx = [i for i, error in enumerate(errors_rmse) if error > 1.5*mean(errors_rmse)]\n",
    "outlier_times = sorted([(test_times[i], errors_rmse[i]) for i in idx])\n",
    "print(idx, outlier_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "weights = np.ones_like(errors_rmse) / (len(errors_rmse))\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "ax1.hist(errors_rmse, bins = 20, range=(0.5, 9), edgecolor = 'black', weights= weights)\n",
    "ax1.set_xlabel('Average RMSE for Each Selected Day')\n",
    "ax1.set_ylabel('Percentage')\n",
    "ax1.set_ylim(0, 0.27)\n",
    "ax2.hist(errors_mae, bins = 20, range=(0.5, 9), edgecolor = 'black', weights= weights)\n",
    "ax2.set_xlabel('Average MAE for Each Selected Day')\n",
    "ax2.set_ylabel('Percentage')\n",
    "ax1.set_ylim(0, 0.27)\n",
    "plt.savefig('HistRMSEDays.jpeg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "testScores_stations = dict()\n",
    "idx = [error < 1.5*mean(errors_rmse) for error in errors_rmse]\n",
    "testScores_stations, test_min_, test_mean_, test_max_  = stat_scores(test_pred_orig_dict, idx)\n",
    "print(\"Test Stations with min, mean and max RMSE are %s, %s, %s\" % (test_min_, test_mean_, test_max_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save predicted result to nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# test_stations are not shuffled\n",
    "outputStations = stations[stations['geohash'].isin(list(test_pred_orig_dict.keys()))]\n",
    "list(outputStations['geohash'])== list(test_pred_orig_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "outputStations.to_csv('/media/fbx5002/CodeMonkey/IEE/IoT_Weather_Data/deliverables/stations.csv', index=False)\n",
    "\n",
    "np.save('/media/fbx5002/CodeMonkey/IEE/IoT_Weather_Data/deliverables/selectedDates.npy', test_times)\n",
    "np.save('/media/fbx5002/CodeMonkey/IEE/IoT_Weather_Data/deliverables/predictions.npy', \n",
    "        np.array([x[0].numpy() for x in list(test_pred_orig_dict.values())]))\n",
    "np.save('/media/fbx5002/CodeMonkey/IEE/IoT_Weather_Data/deliverables/observations.npy', \n",
    "        np.array([x[1].numpy() for x in list(test_pred_orig_dict.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
